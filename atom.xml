<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Leeking&#39;s Blog</title>
  
  <subtitle>不忘初心 方得始终</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-11-10T13:43:24.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Leeking</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2018/11/10/1/"/>
    <id>http://yoursite.com/2018/11/10/1/</id>
    <published>2018-11-10T13:34:04.260Z</published>
    <updated>2018-11-10T13:43:24.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>使用github pages服务搭建博客的好处有：</p><ol><li>全是静态文件，访问速度快；</li><li>免费方便，不用花一分钱就可以搭建一个自由的个人博客，不需要服务器不需要后台；</li><li>可以随意绑定自己的域名，不仔细看的话根本看不出来你的网站是基于github的；</li></ol><a id="more"></a><ol start="4"><li>数据绝对安全，基于github的版本管理，想恢复到哪个历史版本都行；</li><li>博客内容可以轻松打包、转移、发布到其它平台；</li><li>等等；</li></ol><hr>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;使用github pages服务搭建博客的好处有：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;全是静态文件，访问速度快；&lt;/li&gt;
&lt;li&gt;免费方便，不用花一分钱就可以搭建一个自由的个人博客，不需要服务器不需要后台；&lt;/li&gt;
&lt;li&gt;可以随意绑定自己的域名，不仔细看的话根本看不出来你的网站是基于github的；&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Elasticsearch 架构以及源码概览</title>
    <link href="http://yoursite.com/2016/02/10/Elasticsearch%20%E6%9E%B6%E6%9E%84%E4%BB%A5%E5%8F%8A%E6%BA%90%E7%A0%81%E6%A6%82%E8%A7%88/"/>
    <id>http://yoursite.com/2016/02/10/Elasticsearch 架构以及源码概览/</id>
    <published>2016-02-10T13:03:14.000Z</published>
    <updated>2018-12-08T04:25:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>Elasticsearch 架构以及源码概览<br><a id="more"></a></p><h1 id="Elasticsearch-中的几个概念"><a href="#Elasticsearch-中的几个概念" class="headerlink" title="Elasticsearch 中的几个概念"></a>Elasticsearch 中的几个概念</h1><ol><li>集群（Cluster）一组拥有共同的 cluster name 的节点。</li><li>节点（Node) 集群中的一个 Elasticearch 实例。</li><li>索引（Index) 相当于关系数据库中的database概念，一个集群中可以包含多个索引。这个是个逻辑概念。</li><li>主分片（Primary shard） 索引的子集，索引可以切分成多个分片，分布到不同的集群节点上。分片对应的是 Lucene 中的索引。</li><li>副本分片（Replica shard）每个主分片可以有一个或者多个副本。</li><li>类型（Type）相当于数据库中的table概念，mapping是针对 Type 的。同一个索引里可以包含多个 Type。</li><li>Mapping 相当于数据库中的schema，用来约束字段的类型，不过 Elasticsearch 的 mapping 可以自动根据数据创建。</li><li>文档（Document) 相当于数据库中的row。</li><li>字段（Field）相当于数据库中的column。</li><li>分配（Allocation） 将分片分配给某个节点的过程，包括分配主分片或者副本。如果是副本，还包含从主分片复制数据的过程。<h1 id="分布式以及-Elastic"><a href="#分布式以及-Elastic" class="headerlink" title="分布式以及 Elastic"></a>分布式以及 Elastic</h1>分布式系统要解决的第一个问题就是节点之间互相发现以及选主的机制。如果使用了 Zookeeper/Etcd 这样的成熟的服务发现工具，这两个问题都一并解决了。但 Elasticsearch 并没有依赖这样的工具，带来的好处是部署服务的成本和复杂度降低了，不用预先依赖一个服务发现的集群，缺点当然是将复杂度带入了 Elasticsearch 内部。<h2 id="服务发现以及选主-ZenDiscovery"><a href="#服务发现以及选主-ZenDiscovery" class="headerlink" title="服务发现以及选主 ZenDiscovery"></a>服务发现以及选主 ZenDiscovery</h2></li><li>节点启动后先ping（这里的ping是 Elasticsearch 的一个RPC命令。如果 discovery.zen.ping.unicast.hosts 有设置，则ping设置中的host，否则尝试ping localhost 的几个端口， Elasticsearch 支持同一个主机启动多个节点）</li><li>Ping的response会包含该节点的基本信息以及该节点认为的master节点。</li><li>选举开始，先从各节点认为的master中选，规则很简单，按照id的字典序排序，取第一个。</li><li>如果各节点都没有认为的master，则从所有节点中选择，规则同上。这里有个限制条件就是 discovery.zen.minimum_master_nodes，如果节点数达不到最小值的限制，则循环上述过程，直到节点数足够可以开始选举。</li><li>最后选举结果是肯定能选举出一个master，如果只有一个local节点那就选出的是自己。</li><li>如果当前节点是master，则开始等待节点数达到 minimum_master_nodes，然后提供服务。</li><li>如果当前节点不是master，则尝试加入master。</li></ol><p>Elasticsearch 将以上服务发现以及选主的流程叫做 ZenDiscovery 。由于它支持任意数目的集群（1-N）,所以不能像 Zookeeper/Etcd 那样限制节点必须是奇数，也就无法用投票的机制来选主，而是通过一个规则，只要所有的节点都遵循同样的规则，得到的信息都是对等的，选出来的主节点肯定是一致的。但分布式系统的问题就出在信息不对等的情况，这时候很容易出现脑裂（Split-Brain）的问题，大多数解决方案就是设置一个quorum值，要求可用节点必须大于quorum（一般是超过半数节点），才能对外提供服务。而 Elasticsearch 中，这个quorum的配置就是 discovery.zen.minimum_master_nodes 。 说到这里要吐槽下 Elasticsearch 的方法和变量命名，它的方法和配置中的master指的是master的候选节点，也就是说可能成为master的节点，并不是表示当前的master，我就被它的一个 isMasterNode 方法坑了，开始一直没能理解它的选举规则。</p><h2 id="弹性伸缩-Elastic"><a href="#弹性伸缩-Elastic" class="headerlink" title="弹性伸缩 Elastic"></a>弹性伸缩 Elastic</h2><h3 id="Elasticsearch-的弹性体现在两个方面："><a href="#Elasticsearch-的弹性体现在两个方面：" class="headerlink" title="Elasticsearch 的弹性体现在两个方面："></a>Elasticsearch 的弹性体现在两个方面：</h3><ol><li>服务发现机制让节点很容易加入和退出。</li><li>丰富的设置以及allocation API。</li></ol><p>Elasticsearch 节点启动的时候只需要配置discovery.zen.ping.unicast.hosts，这里不需要列举集群中所有的节点，只要知道其中一个即可。当然为了避免重启集群时正好配置的节点挂掉，最好多配置几个节点。节点退出时只需要调用 API 将该节点从集群中排除 （Shard Allocation Filtering），系统会自动迁移该节点上的数据，然后关闭该节点即可。当然最好也将不可用的已知节点从其他节点的配置中去除，避免下次启动时出错。</p><h2 id="分片（Shard）以及副本（Replica）"><a href="#分片（Shard）以及副本（Replica）" class="headerlink" title="分片（Shard）以及副本（Replica）"></a>分片（Shard）以及副本（Replica）</h2><p>分布式存储系统为了解决单机容量以及容灾的问题，都需要有分片以及副本机制。Elasticsearch 没有采用节点级别的主从复制，而是基于分片。它当前还未提供分片切分（shard-splitting）的机制，只能创建索引的时候静态设置。</p><p>开始设置为5个分片，在单个节点上，后来扩容到5个节点，每个节点有一个分片。如果继续扩容，是不能自动切分进行数据迁移的。官方文档的说法是分片切分成本和重新索引的成本差不多，所以建议干脆通过接口重新索引。</p><p>Elasticsearch 的分片默认是基于id 哈希的，id可以用户指定，也可以自动生成。但这个可以通过参数（routing）或者在mapping配置中修改。当前版本默认的哈希算法是MurmurHash3。</p><p>Elasticsearch 禁止同一个分片的主分片和副本分片在同一个节点上，所以如果是一个节点的集群是不能有副本的。</p><h2 id="恢复以及容灾"><a href="#恢复以及容灾" class="headerlink" title="恢复以及容灾"></a>恢复以及容灾</h2><p>分布式系统的一个要求就是要保证高可用。前面描述的退出流程是节点主动退出的场景，但如果是故障导致节点挂掉，Elasticsearch 就会主动allocation。但如果节点丢失后立刻allocation，稍后节点恢复又立刻加入，会造成浪费。Elasticsearch的恢复流程大致如下：</p><ol><li><ul><li>集群中的某个节点丢失网络连接</li></ul></li><li><ul><li>master提升该节点上的所有主分片的在其他节点上的副本为主分片</li></ul></li><li><ul><li>cluster集群状态变为 yellow ,因为副本数不够</li></ul></li><li><ul><li>等待一个超时设置的时间，如果丢失节点回来就可以立即恢复（默认为1分钟，通过 index.unassigned.node_left.delayed_timeout 设置）。如果该分片已经有写入，则通过translog进行增量同步数据。</li></ul></li><li><ul><li>否则将副本分配给其他节点，开始同步数据。</li></ul></li></ol><p>但如果该节点上的分片没有副本，则无法恢复，集群状态会变为red，表示可能要丢失该分片的数据了。</p><p>分布式集群的另外一个问题就是集群整个重启后可能导致不预期的分片重新分配（部分节点没有启动完成的时候，集群以为节点丢失），浪费带宽。所以 Elasticsearch 通过以下静态配置（不能通过API修改）控制整个流程，以10个节点的集群为例：</p><ul><li>gateway.recover_after_nodes: 8</li><li>gateway.expected_nodes: 10</li><li>gateway.recover_after_time: 5m</li></ul><p>比如10个节点的集群，按照上面的规则配置，当集群重启后，首先系统等待 minimum_master_nodes（6）个节点加入才会选出master， recovery操作是在 master节点上进行的，由于我们设置了 recover_after_nodes（8），系统会继续等待到8个节点加入， 才开始进行recovery。当开始recovery的时候，如果发现集群中的节点数小于expected_nodes，也就是还有部分节点未加入，于是开始recover_after_time 倒计时(如果节点数达到expected_nodes则立刻进行 recovery)，5分钟后，如果剩余的节点依然没有加入，则会进行数据recovery。</p><h1 id="搜索引擎-Search"><a href="#搜索引擎-Search" class="headerlink" title="搜索引擎 Search"></a>搜索引擎 Search</h1><p>Elasticsearch 除了支持 Lucene 本身的检索功能外，在之上做了一些扩展。 1. 脚本支持<br>Elasticsearch 默认支持groovy脚本，扩展了 Lucene 的评分机制，可以很容易的支持复杂的自定义评分算法。它默认只支持通过sandbox方式实现的脚本语言（如lucene expression，mustache），groovy必须明确设置后才能开启。Groovy的安全机制是通过java.security.AccessControlContext设置了一个class白名单来控制权限的，1.x版本的时候是自己做的一个白名单过滤器，但限制策略有漏洞，导致一个远程代码执行漏洞。 2. 默认会生成一个 _all 字段，将所有其他字段的值拼接在一起。这样搜索时可以不指定字段，并且方便实现跨字段的检索。 3. Suggester Elasticsearch 通过扩展的索引机制，可以实现像google那样的自动完成suggestion以及搜索词语错误纠正的suggestion。</p><h1 id="NoSQL-数据库"><a href="#NoSQL-数据库" class="headerlink" title="NoSQL 数据库"></a>NoSQL 数据库</h1><h2 id="Elasticsearch-可以作为数据库使用，主要依赖于它的以下特性："><a href="#Elasticsearch-可以作为数据库使用，主要依赖于它的以下特性：" class="headerlink" title="Elasticsearch 可以作为数据库使用，主要依赖于它的以下特性："></a>Elasticsearch 可以作为数据库使用，主要依赖于它的以下特性：</h2><ul><li>默认在索引中保存原始数据，并可获取。这个主要依赖 Lucene 的store功能。</li><li>实现了translog，提供了实时的数据读取能力以及完备的数据持久化能力（在服务器异常挂掉的情况下依然不会丢数据）。Lucene 因为有 IndexWriter buffer, 如果进程异常挂掉，buffer中的数据是会丢失的。所以 Elasticsearch 通过translog来确保不丢数据。同时通过id直接读取文档的时候，Elasticsearch 会先尝试从translog中读取，之后才从索引中读取。也就是说，即便是buffer中的数据尚未刷新到索引，依然能提供实时的数据读取能力。Elasticsearch 的translog 默认是每次写请求完成后统一fsync一次，同时有个定时任务检测（默认5秒钟一次）。如果业务场景需要更大的写吞吐量，可以调整translog相关的配置进行优化。</li><li>dynamic-mapping 以及 schema-free</li><li>Elasticsearch 的dynamic-mapping相当于根据用户提交的数据，动态检测字段类型，自动给数据库表建立表结构，也可以动态增加字段，所以它叫做schema-free，而不是schema-less。这种方式的好处是用户能一定程度享受schema-less的好处，不用提前建立表结构，同时因为实际上是有schema的，可以做查询上的优化，检索效率要比纯schema-less的数据库高许多。但缺点就是已经创建的索引不能变更数据类型（Elasticsearch 写入数据的时候如果类型不匹配会自动尝试做类型转换，如果失败就会报错，比如数字类型的字段写入字符串”123”是可以的，但写入”abc”就不可以。），要损失一定的自由度。另外 Elasticsearch 提供的index-template功能方便用户动态创建索引的时候预先设定索引的相关参数以及type mapping，比如按天创建日志库，template可以设置为对 log-* 的索引都生效。这两个功能我建议新的数据库都可以借鉴下。</li><li>丰富的QueryDSL功能</li><li>Elasticsearch 的query语法基本上和sql对等的，除了join查询，以及嵌套临时表查询不能支持。不过 Elasticsearch 支持嵌套对象以及parent外部引用查询，所以一定程度上可以解决关联查询的需求。另外group by这种查询可以通过其aggregation实现。Elasticsearch 提供的aggregation能力非常强大，其生态圈里的 Kibana 主要就是依赖aggregation来实现数据分析以及可视化的。</li></ul><h1 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h1><p>Elasticsearch 的依赖注入用的是guice，网络使用netty，提供http rest和RPC两种协议。</p><h2 id="guice"><a href="#guice" class="headerlink" title="guice"></a>guice</h2><p>Elasticsearch 之所以用guice，而不是用spring做依赖注入，关键的一个原因是guice可以帮它很容易的实现模块化，通过代码进行模块组装，可以很精确的控制依赖注入的管理范围。比如 Elasticsearch 给每个shard单独生成一个injector，可以将该shard相关的配置以及组件注入进去，降低编码和状态管理的复杂度，同时删除shard的时候也方便回收相关对象。这方面有兴趣使用guice的可以借鉴。</p><h2 id="ClusterState"><a href="#ClusterState" class="headerlink" title="ClusterState"></a>ClusterState</h2><p>前面我们分析了 Elasticsearch 的服务发现以及选举机制，它是内部自己实现的。服务发现工具做的事情其实就是跨服务器的状态同步，多个节点修改同一个数据对象，需要有一种机制将这个数据对象同步到所有的节点。Elasticsearch 的ClusterState 就是这样一个数据对象，保存了集群的状态，索引/分片的路由表，节点列表，元数据等，还包含一个ClusterBlocks，相当于分布式锁，用于实现分布式的任务同步。</p><p>主节点上有个单独的进程处理 ClusterState 的变更操作，每次变更会更新版本号。变更后会通过PRC接口同步到其他节点。主节知道其他节点的ClusterState 的当前版本，发送变更的时候会做diff，实现增量更新。</p><p>Rest 和 RPC</p><ol><li>用户发起http请求，Elasticsearch 的9200端口接受请求后，传递给对应的RestAction。 </li><li>RestAction做的事情很简单，将rest请求转换为RPC的TransportRequest，然后调用NodeClient，相当于用客户端的方式请求RPC服务，只不过transport层会对本节点的请求特殊处理。</li></ol><p>这样做的好处是将http和RPC两层隔离，增加部署的灵活性。部署的时候既可以同时开启RPC和http服务，也可以用client模式部署一组服务专门提供http rest服务，另外一组只开启RPC服务，专门做data节点，便于分担压力。</p><p>Elasticsearch 的RPC的序列化机制使用了 Lucene 的压缩数据类型，支持vint这样的变长数字类型，省略了字段名，用流式方式按顺序写入字段的值。每个需要传输的对象都需要实现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1 void writeTo(StreamOutput out) </span><br><span class="line">2 T readFrom(StreamInput in)</span><br></pre></td></tr></table></figure><p>两个方法。虽然这样实现开发成本略高，增删字段也不太灵活，但对 Elasticsearch 这样的数据库系统来说，不用考虑跨语言，增删字段肯定要考虑兼容性，这样做效率最高。所以 Elasticsearch 的RPC接口只有java client可以直接请求，其他语言的客户端都走的是rest接口。</p><h2 id="网络层"><a href="#网络层" class="headerlink" title="网络层"></a>网络层</h2><p>Elasticsearch 的网络层抽象很值得借鉴。它抽象出一个 Transport 层，同时兼有client和server功能，server端接收其他节点的连接，client维持和其他节点的连接，承担了节点之间请求转发的功能。Elasticsearch 为了避免传输流量比较大的操作堵塞连接，所以会按照优先级创建多个连接，称为channel。</p><ol><li>recovery: 2个channel专门用做恢复数据。如果为了避免恢复数据时将带宽占满，还可以设置恢复数据时的网络传输速度。</li><li>bulk: 3个channel用来传输批量请求等基本比较低的请求。</li><li>regular: 6个channel用来传输通用正常的请求，中等级别。</li><li>state: 1个channel保留给集群状态相关的操作，比如集群状态变更的传输，高级别。</li><li>ping: 1个channel专门用来ping，进行故障检测。</li></ol><p>每个节点默认都会创建13个到其他节点的连接，并且节点之间是互相连接的，每增加一个节点，该节点会到每个节点创建13个连接，而其他每个节点也会创建13个连回来的连接。</p><p>线程池</p><p>它默认创建了10多个线程池，按照不同的优先级以及不同的操作进行划分。然后提供了4种类型的线程池，不同的线程池使用不同的类型：</p><ul><li>CACHED 最小为0，无上限，无队列（SynchronousQueue，没有缓冲buffer），有存活时间检测的线程池。通用的，希望能尽可能支撑的任务。</li><li>DIRECT 直接在调用者的线程里执行，其实这不算一种线程池方案，主要是为了代码逻辑上的统一而创造的一种线程类型。</li><li>FIXED 固定大小的线程池，带有缓冲队列。用于计算和IO的耗时波动较小的操作。</li><li>SCALING 有最小值，最大值的伸缩线程池，队列是基于LinkedTransferQueue 改造的实现，和java内置的Executors生成的伸缩线程池的区别是优先增加线程，增加到最大值后才会使用队列，和java内置的线程池规则相反。用于计算和IO耗时都不太稳定，需要限制系统承载最大任务上限的操作。</li></ul><p>这种解决方案虽然要求每个用到线程池的地方都需要评估下执行成本以及应该用什么样的线程池，但好处是限制了线程池的泛滥，也缓解了不同类型的任务互相之间的影响。</p><p><a href="http://www.code123.cc/2582.html" target="_blank" rel="noopener">http://www.code123.cc/2582.html</a></p>]]></content>
    
    <summary type="html">
    
      Elasticsearch 架构以及源码概览
    
    </summary>
    
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Spark知识体系完整解读</title>
    <link href="http://yoursite.com/2015/07/10/Spark%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E5%AE%8C%E6%95%B4%E8%A7%A3%E8%AF%BB/"/>
    <id>http://yoursite.com/2015/07/10/Spark知识体系完整解读/</id>
    <published>2015-07-10T07:24:52.000Z</published>
    <updated>2018-12-08T04:25:54.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Spark简介"><a href="#Spark简介" class="headerlink" title="Spark简介"></a>Spark简介</h1><p>　　Spark是整个BDAS的核心组件，是一个大数据分布式编程框架，不仅实现了MapReduce的算子map 函数和reduce函数及计算模型，还提供更为丰富的算子，如filter、join、groupByKey等。是一个用来实现快速而同用的集群计算的平台。</p><p>　　Spark将分布式数据抽象为弹性分布式数据集（RDD），实现了应用任务调度、RPC、序列化和压缩，并为运行在其上的上层组件提供API。其底层采用Scala这种函数式语言书写而成，并且所提供的API深度借鉴Scala函数式的编程思想，提供与Scala类似的编程接口<br><a id="more"></a></p><h2 id="Sparkon-Yarn"><a href="#Sparkon-Yarn" class="headerlink" title="Sparkon Yarn"></a>Sparkon Yarn</h2><p><img src="http://img.mp.itc.cn/upload/20160523/b6fc307323e9498785a2651633dbc135_th.jpg" alt="image"></p><p>从用户提交作业到作业运行结束整个运行期间的过程分析。</p><h2 id="一、客户端进行操作"><a href="#一、客户端进行操作" class="headerlink" title="一、客户端进行操作"></a>一、客户端进行操作</h2><ol><li>根据yarnConf来初始化yarnClient，并启动yarnClient</li><li><p>创建客户端Application，并获取Application的ID，进一步判断集群中的资源是否满足executor和ApplicationMaster申请的资源，如果不满足则抛出IllegalArgumentException；</p></li><li><p>设置资源、环境变量：其中包括了设置Application的Staging目录、准备本地资源（jar文件、log4j.properties）、设置Application其中的环境变量、创建Container启动的Context等；</p></li><li><p>设置Application提交的Context，包括设置应用的名字、队列、AM的申请的Container、标记该作业的类型为Spark；</p></li><li><p>申请Memory，并最终通过yarnClient.submitApplication向ResourceManager提交该Application。</p></li></ol><p>　　当作业提交到YARN上之后，客户端就没事了，甚至在终端关掉那个进程也没事，因为整个作业运行在YARN集群上进行，运行的结果将会保存到HDFS或者日志中。　　</p><h2 id="二、提交到YARN集群，YARN操作"><a href="#二、提交到YARN集群，YARN操作" class="headerlink" title="二、提交到YARN集群，YARN操作"></a>二、提交到YARN集群，YARN操作</h2><ol><li><p>运行ApplicationMaster的run方法；</p></li><li><p>设置好相关的环境变量。</p></li><li><p>创建amClient，并启动；</p></li><li><p>在Spark UI启动之前设置Spark UI的AmIpFilter；</p></li><li><p>在startUserClass函数专门启动了一个线程（名称为Driver的线程）来启动用户提交的Application，也就是启动了Driver。在Driver中将会初始化SparkContext；</p></li><li><p>等待SparkContext初始化完成，最多等待spark.yarn.applicationMaster.waitTries次数（默认为10），如果等待了的次数超过了配置的，程序将会退出；否则用SparkContext初始化yarnAllocator；</p></li><li><p>当SparkContext、Driver初始化完成的时候，通过amClient向ResourceManager注册ApplicationMaster</p></li><li><p>分配并启动Executeors。在启动Executeors之前，先要通过yarnAllocator获取到numExecutors个Container，然后在Container中启动Executeors。</p></li><li><p>　　那么这个Application将失败，将Application Status标明为FAILED，并将关闭SparkContext。其实，启动Executeors是通过ExecutorRunnable实现的，而ExecutorRunnable内部是启动CoarseGrainedExecutorBackend的。</p></li><li><p>最后，Task将在CoarseGrainedExecutorBackend里面运行，然后运行状况会通过Akka通知CoarseGrainedScheduler，直到作业运行完成。</p></li></ol><h1 id="Spark节点的概念"><a href="#Spark节点的概念" class="headerlink" title="Spark节点的概念"></a>Spark节点的概念</h1><h2 id="一、Spark驱动器是执行程序中的main-方法的进程。"><a href="#一、Spark驱动器是执行程序中的main-方法的进程。" class="headerlink" title="一、Spark驱动器是执行程序中的main()方法的进程。"></a>一、Spark驱动器是执行程序中的main()方法的进程。</h2><p> 它执行用户编写的用来创建SparkContext(初始化)、创建RDD，以及运行RDD的转化操作和行动操作的代码。</p><h3 id="驱动器节点driver的职责："><a href="#驱动器节点driver的职责：" class="headerlink" title="驱动器节点driver的职责："></a>驱动器节点driver的职责：</h3><ol><li><p>把用户程序转为任务task(driver)</p><p>Spark驱动器程序负责把用户程序转化为多个物理执行单元，这些单元也被称之为任务task(详解见备注)</p></li><li><p>为执行器节点调度任务(executor)</p></li></ol><p>　　有了物理计划之后，Spark驱动器在各个执行器节点进程间协调任务的调度。Spark驱动器程序会根据当前的执行器节点，把所有任务基于数据所在位置分配给合适的执行器进程。当执行任务时，执行器进程会把缓存的数据存储起来，而驱动器进程同样会跟踪这些缓存数据的位置，并利用这些位置信息来调度以后的任务，以尽量减少数据的网络传输。（就是所谓的移动计算，而不移动数据)。　　</p><h2 id="二、执行器节点"><a href="#二、执行器节点" class="headerlink" title="二、执行器节点"></a>二、执行器节点</h2><h3 id="作用："><a href="#作用：" class="headerlink" title="作用："></a>作用：</h3><ol><li><p>负责运行组成Spark应用的任务，并将结果返回给驱动器进程；</p></li><li><p>通过自身的块管理器(blockManager)为用户程序中要求缓存的RDD提供内存式存储。RDD是直接缓存在执行器进程内的，因此任务可以在运行时充分利用缓存数据加快运算。</p></li></ol><p>　　驱动器的职责：</p><p>　　所有的Spark程序都遵循同样的结构：程序从输入数据创建一系列RDD，再使用转化操作派生成新的RDD，最后使用行动操作手机或存储结果RDD，Spark程序其实是隐式地创建出了一个由操作组成的逻辑上的有向无环图DAG。当驱动器程序执行时，它会把这个逻辑图转为物理执行计划。</p><p>　　这样 Spark就把逻辑计划转为一系列步骤(stage)，而每个步骤又由多个任务组成。这些任务会被打包送到集群中。　　</p><h3 id="Spark初始化"><a href="#Spark初始化" class="headerlink" title="Spark初始化"></a>Spark初始化</h3><ol><li><p>每个Spark应用都由一个驱动器程序来发起集群上的各种并行操作。驱动器程序包含应用的main函数，并且定义了集群上的分布式数据集，以及对该分布式数据集应用了相关操作。</p></li><li><p>驱动器程序通过一个SparkContext对象来访问spark,这个对象代表对计算集群的一个连接。（比如在sparkshell启动时已经自动创建了一个SparkContext对象，是一个叫做SC的变量。(下图，查看变量sc)</p></li></ol><p><img src="http://img.mp.itc.cn/upload/20160523/e84eb1ecb347402cbc4fe7c3a1490cca.jpg" alt="image"></p><ol><li>一旦创建了sparkContext，就可以用它来创建RDD。比如调用sc.textFile()来创建一个代表文本中各行文本的RDD。（比如vallinesRDD = sc.textFile(“yangsy.text”),val spark = linesRDD.filter(line=&gt;line.contains(“spark”),spark.count()）</li></ol><p>　　执行这些操作，驱动器程序一般要管理多个执行器,就是我们所说的executor节点。</p><ol><li>在初始化SparkContext的同时，加载sparkConf对象来加载集群的配置，从而创建sparkContext对象。</li></ol><p>　　从源码中可以看到，在启动thriftserver时，调用了spark- daemon.sh文件，该文件源码如左图，加载spark_home下的conf中的文件。<br>　　<br>　　<img src="http://img.mp.itc.cn/upload/20160523/9c8c43b1175c4c9cb5aca38a1651a383_th.jpg" alt="image"><br>　　<br>　　（在执行后台代码时，需要首先创建conf对象，加载相应参数， val sparkConf = newSparkConf().setMaster(“local”).setAppName(“cocapp”).set(“spark.executor.memory”,”1g”), val sc: SparkContext = new SparkContext(sparkConf))　　</p><h2 id="RDD工作原理："><a href="#RDD工作原理：" class="headerlink" title="　　RDD工作原理："></a>　　RDD工作原理：</h2><p>　　RDD(Resilient DistributedDatasets)[1] ,弹性分布式数据集，是分布式内存的一个抽象概念，RDD提供了一种高度受限的共享内存模型，即RDD是只读的记录分区的集合，只能通过在其他RDD执行确定的转换操作（如map、join和group by）而创建，然而这些限制使得实现容错的开销很低。对开发者而言，RDD可以看作是Spark的一个对象，它本身运行于内存中，如读文件是一个RDD，对文件计算是一个RDD，结果集也是一个RDD ，不同的分片、数据之间的依赖、key-value类型的map数据都可以看做RDD。</p><h3 id="主要分为三部分：创建RDD对象，DAG调度器创建执行计划，Task调度器分配任务并调度Worker开始运行。"><a href="#主要分为三部分：创建RDD对象，DAG调度器创建执行计划，Task调度器分配任务并调度Worker开始运行。" class="headerlink" title="　　主要分为三部分：创建RDD对象，DAG调度器创建执行计划，Task调度器分配任务并调度Worker开始运行。"></a>　　主要分为三部分：创建RDD对象，DAG调度器创建执行计划，Task调度器分配任务并调度Worker开始运行。</h3><ol><li><p>　　SparkContext(RDD相关操作)→通过(提交作业)→(遍历RDD拆分stage→生成作业)DAGScheduler→通过（提交任务集）→任务调度管理(TaskScheduler)→通过（按照资源获取任务)→任务调度管理(TaskSetManager)</p></li><li><p>　　Transformation返回值还是一个RDD。它使用了链式调用的设计模式，对一个RDD进行计算后，变换成另外一个RDD，然后这个RDD又可以进行另外一次转换。这个过程是分布式的。</p></li><li><p>　　Action返回值不是一个RDD。它要么是一个Scala的普通集合，要么是一个值，要么是空，最终或返回到Driver程序，或把RDD写入到文件系统中</p></li><li><p>　　转换(Transformations)(如：map, filter, groupBy, join等)，Transformations操作是Lazy的，也就是说从一个RDD转换生成另一个RDD的操作不是马上执行，Spark在遇到Transformations操作时只会记录需要这样的操作，并不会去执行，需要等到有Actions操作的时候才会真正启动计算过程进行计算。</p></li><li><p>　　操作(Actions)(如：count, collect, save等)，Actions操作会返回结果或把RDD数据写到存储系统中。Actions是触发Spark启动计算的动因。</p></li><li><p>　　它们本质区别是：Transformation返回值还是一个RDD。它使用了链式调用的设计模式，对一个RDD进行计算后，变换成另外一个RDD，然后这个RDD又可以进行另外一次转换。这个过程是分布式的。Action返回值不是一个RDD。它要么是一个Scala的普通集合，要么是一个值，要么是空，最终或返回到Driver程序，或把RDD写入到文件系统中。关于这两个动作，在Spark开发指南中会有就进一步的详细介绍，它们是基于Spark开发的核心。</p></li></ol><h2 id="RDD基础"><a href="#RDD基础" class="headerlink" title="　　RDD基础"></a>　　RDD基础</h2><ol><li><p>Spark中的RDD就是一个不可变的分布式对象集合。每个RDD都被分为多个分区，这些分区运行在集群的不同节点上。创建RDD的方法有两种：一种是读取一个外部数据集；一种是在群东程序里分发驱动器程序中的对象集合，不如刚才的示例，读取文本文件作为一个字符串的RDD的示例。</p></li><li><p>创建出来后，RDD支持两种类型的操作:转化操作和行动操作</p></li></ol><p>　　转化操作会由一个RDD生成一个新的RDD。（比如刚才的根据谓词筛选）</p><p>　　行动操作会对RDD计算出一个结果，并把结果返回到驱动器程序中，或把结果存储到外部存储系统（比如HDFS）中。比如first()操作就是一个行动操作，会返回RDD的第一个元素。</p><p>　　注：转化操作与行动操作的区别在于Spark计算RDD的方式不同。虽然你可以在任何时候定义一个新的RDD，但Spark只会惰性计算这些RDD。它们只有第一个在一个行动操作中用到时，才会真正的计算。之所以这样设计，是因为比如刚才调用sc.textFile(…)时就把文件中的所有行都读取并存储起来，就会消耗很多存储空间，而我们马上又要筛选掉其中的很多数据。</p><p>　　这里还需要注意的一点是，spark会在你每次对它们进行行动操作时重新计算。如果想在多个行动操作中重用同一个RDD，那么可以使用RDD.persist()或RDD.collect()让Spark把这个RDD缓存下来。（可以是内存，也可以是磁盘)</p><ol><li>Spark会使用谱系图来记录这些不同RDD之间的依赖关系，Spark需要用这些信息来按需计算每个RDD，也可以依靠谱系图在持久化的RDD丢失部分数据时用来恢复所丢失的数据。(如下图，过滤errorsRDD与warningsRDD,最终调用union()函数)</li></ol><p><img src="http://img.mp.itc.cn/upload/20160523/97757ec3b2ab4d56b0942b96e300c71d.jpg" alt="image"></p><p>RDD计算方式</p><p><img src="http://img.mp.itc.cn/upload/20160523/06d3d32ab45742e68e7683672f6a46a5_th.jpg" alt="image"></p><p>RDD的宽窄依赖</p><p><img src="http://img.mp.itc.cn/upload/20160523/c8c6f04edfc14bb79dffd997e1de2e5a_th.jpg" alt="image"></p><p>窄依赖 (narrowdependencies) 和宽依赖 (widedependencies) 。窄依赖是指 父 RDD 的每个分区都只被子 RDD 的一个分区所使用 。相应的，那么宽依赖就是指父 RDD 的分区被多个子 RDD 的分区所依赖。例如， map 就是一种窄依赖，而 join 则会导致宽依赖</p><p>　　这种划分有两个用处。首先，窄依赖支持在一个结点上管道化执行。例如基于一对一的关系，可以在 filter 之后执行 map 。其次，窄依赖支持更高效的故障还原。因为对于窄依赖，只有丢失的父 RDD 的分区需要重新计算。而对于宽依赖，一个结点的故障可能导致来自所有父 RDD 的分区丢失，因此就需要完全重新执行。因此对于宽依赖，Spark 会在持有各个父分区的结点上，将中间数据持久化来简化故障还原，就像 MapReduce 会持久化 map 的输出一样。　　</p><ul><li><p>　　步骤 1 ：创建 RDD 。上面的例子除去最后一个 collect 是个动作，不会创建 RDD 之外，前面四个转换都会创建出新的 RDD 。因此第一步就是创建好所有 RDD( 内部的五项信息 ) 。</p></li><li><p>　　步骤 2 ：创建执行计划。Spark 会尽可能地管道化，并基于是否要重新组织数据来划分 阶段 (stage) ，例如本例中的 groupBy() 转换就会将整个执行计划划分成两阶段执行。最终会产生一个 DAG(directedacyclic graph ，有向无环图 ) 作为逻辑执行计划。</p></li><li><p>　　步骤 3 ：调度任务。 将各阶段划分成不同的 任务 (task) ，每个任务都是数据和计算的合体。在进行下一阶段前，当前阶段的所有任务都要执行完成。因为下一阶段的第一个转换一定是重新组织数据的，所以必须等当前阶段所有结果数据都计算出来了才能继续。</p></li></ul><p>　　假设本例中的 hdfs://names 下有四个文件块，那么 HadoopRDD 中 partitions 就会有四个分区对应这四个块数据，同时 preferedLocations 会指明这四个块的最佳位置。现在，就可以创建出四个任务，并调度到合适的集群结点上。　　</p><h2 id="Spark数据分区"><a href="#Spark数据分区" class="headerlink" title="Spark数据分区"></a>Spark数据分区</h2><ol><li><p>Spark的特性是对数据集在节点间的分区进行控制。在分布式系统中，通讯的代价是巨大的，控制数据分布以获得最少的网络传输可以极大地提升整体性能。Spark程序可以通过控制RDD分区方式来减少通讯的开销。</p></li><li><p>Spark中所有的键值对RDD都可以进行分区。确保同一组的键出现在同一个节点上。比如，使用哈希分区将一个RDD分成了100个分区，此时键的哈希值对100取模的结果相同的记录会被放在一个节点上。</p></li></ol><p>　　（可使用partitionBy(newHashPartitioner(100)).persist()来构造100个分区)</p><ol><li>Spark中的许多操作都引入了将数据根据键跨界点进行混洗的过程。(比如：join(),leftOuterJoin(),groupByKey(),reducebyKey()等)对于像reduceByKey()这样只作用于单个RDD的操作，运行在未分区的RDD上的时候会导致每个键的所有对应值都在每台机器上进行本地计算。</li></ol><p>SparkSQL的shuffle过程<br><img src="http://img.mp.itc.cn/upload/20160523/511ccf9cf53e4991a7ac7660dea148ed_th.jpg" alt="image"></p><p>Spark SQL的核心是把已有的RDD，带上Schema信息，然后注册成类似sql里的”Table”，对其进行sql查询。这里面主要分两部分，一是生成SchemaRD，二是执行查询。</p><p>　　如果是spark-hive项目，那么读取metadata信息作为Schema、读取hdfs上数据的过程交给Hive完成，然后根据这俩部分生成SchemaRDD，在HiveContext下进行hql()查询。　　</p><h3 id="SparkSQL结构化数据"><a href="#SparkSQL结构化数据" class="headerlink" title="SparkSQL结构化数据"></a>SparkSQL结构化数据</h3><ol><li><p>首先说一下ApacheHive，Hive可以在HDFS内或者在其他存储系统上存储多种格式的表。SparkSQL可以读取Hive支持的任何表。要把Spark SQL连接已有的hive上，需要提供Hive的配置文件。hive-site.xml文件复制到spark的conf文件夹下。再创建出HiveContext对象(sparksql的入口)，然后就可以使用HQL来对表进行查询，并以由行足证的RDD的形式拿到返回的数据。</p></li><li><p>创建Hivecontext并查询数据</p></li></ol><p>　　importorg.apache.spark.sql.hive.HiveContext</p><p>　　valhiveCtx = new org.apache.spark.sql.hive.HiveContext(sc)</p><p>　　valrows = hiveCtx.sql(“SELECT name,age FROM users”)</p><p>　　valfitstRow – rows.first()</p><p>　　println(fitstRow.getSgtring(0)) //字段0是name字段</p><ol><li>通过jdbc连接外部数据源更新与加载</li></ol><p>　　Class.forName(“com.mysql.jdbc.Driver”)</p><p>　　val conn =DriverManager.getConnection(mySQLUrl)</p><p>　　val stat1 =conn.createStatement()</p><p>　　stat1.execute(“UPDATE CI_LABEL_INFO set DATA_STATUS_ID = 2 , DATA_DATE =’” + dataDate +”‘ where LABEL_ID in (“+allCreatedLabels.mkString(“,”)+”)”)</p><p>　　stat1.close()</p><p>　　//加载外部数据源数据到内存</p><p>　　valDIM_COC_INDEX_MODEL_TABLE_CONF =sqlContext.jdbc(mySQLUrl,”DIM_COC_INDEX_MODEL_TABLE_CONF”).cache()</p><p>　　val targets =DIM_COC_INDEX_MODEL_TABLE_CONF.filter(“TABLE_DATA_CYCLE =”+TABLE_DATA_CYCLE).collect　　</p><h3 id="SparkSQL解析"><a href="#SparkSQL解析" class="headerlink" title="SparkSQL解析"></a>SparkSQL解析</h3><p><img src="http://img.mp.itc.cn/upload/20160523/3e5dbd8210ff4706906ba3609883f9b3_th.jpg" alt="image"></p><p>首先说下传统数据库的解析，传统数据库的解析过程是按Rusult、Data Source、Operation的次序来解析的。传统数据库先将读入的SQL语句进行解析，分辨出SQL语句中哪些词是关键字（如select,from,where)，哪些是表达式，哪些是Projection，哪些是Data Source等等。进一步判断SQL语句是否规范，不规范就报错，规范则按照下一步过程绑定（Bind)。过程绑定是将SQL语句和数据库的数据字典(列,表,视图等）进行绑定，如果相关的Projection、Data Source等都存在，就表示这个SQL语句是可以执行的。在执行过程中，有时候甚至不需要读取物理表就可以返回结果，比如重新运行刚运行过的SQL语句，直接从数据库的缓冲池中获取返回结果。在数据库解析的过程中SQL语句时，将会把SQL语句转化成一个树形结构来进行处理，会形成一个或含有多个节点(TreeNode)的Tree,然后再后续的处理政对该Tree进行一系列的操作。</p><p>　　Spark SQL对SQL语句的处理和关系数据库对SQL语句的解析采用了类似的方法，首先会将SQL语句进行解析，然后形成一个Tree，后续如绑定、优化等处理过程都是对Tree的操作，而操作方法是采用Rule,通过模式匹配，对不同类型的节点采用不同的操作。SparkSQL有两个分支，sqlContext和hiveContext。sqlContext现在只支持SQL语法解析器（Catalyst)，hiveContext支持SQL语法和HiveContext语法解析器。</p>]]></content>
    
    <summary type="html">
    
      Spark知识体系完整解读
    
    </summary>
    
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>HBase框架简单介绍</title>
    <link href="http://yoursite.com/2014/07/10/HBase%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84%E5%8F%8A%E5%8E%9F%E7%90%86/"/>
    <id>http://yoursite.com/2014/07/10/HBase基本架构及原理/</id>
    <published>2014-07-10T14:03:54.000Z</published>
    <updated>2018-12-08T04:25:50.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-HBase框架简单介绍"><a href="#1-HBase框架简单介绍" class="headerlink" title="1. HBase框架简单介绍"></a>1. HBase框架简单介绍</h1><p>HBase是一个分布式的、面向列的开源数据库，它不同于一般的关系数据库,是一个适合于非结构化数据存储的数据库。另一个不同的是HBase基于列的而不是基于行的模式。HBase使用和 BigTable非常相同的数据模型。用户存储数据行在一个表里。一个数据行拥有一个可选择的键和任意数量的列，一个或多个列组成一个ColumnFamily，一个Fmaily下的列位于一个HFile中，易于缓存数据。表是疏松的存储的，因此用户可以给行定义各种不同的列。在HBase中数据按主键排序，同时表按主键划分为多个Region。</p><p>在分布式的生产环境中，HBase 需要运行在 HDFS 之上，以 HDFS 作为其基础的存储设施。HBase 上层提供了访问的数据的 Java API 层，供应用访问存储在 HBase 的数据。在 HBase 的集群中主要由 Master 和 Region Server 组成，以及 Zookeeper，具体模块如下图所示：<br><a id="more"></a><br>简单介绍一下 HBase 中相关模块的作用：</p><h2 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h2><p>HBase Master用于协调多个Region Server，侦测各个RegionServer之间的状态，并平衡RegionServer之间的负载。HBaseMaster还有一个职责就是负责分配Region给RegionServer。HBase允许多个Master节点共存，但是这需要Zookeeper的帮助。不过当多个Master节点共存时，只有一个Master是提供服务的，其他的Master节点处于待命的状态。当正在工作的Master节点宕机时，其他的Master则会接管HBase的集群。</p><h2 id="Region-Server"><a href="#Region-Server" class="headerlink" title="Region Server"></a>Region Server</h2><p>对于一个RegionServer而言，其包括了多个Region。RegionServer的作用只是管理表格，以及实现读写操作。Client直接连接RegionServer，并通信获取HBase中的数据。对于Region而言，则是真实存放HBase数据的地方，也就说Region是HBase可用性和分布式的基本单位。如果当一个表格很大，并由多个CF组成时，那么表的数据将存放在多个Region之间，并且在每个Region中会关联多个存储的单元（Store）。</p><h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><p>对于 HBase 而言，Zookeeper的作用是至关重要的。首先Zookeeper是作为HBase Master的HA解决方案。也就是说，是Zookeeper保证了至少有一个HBase Master 处于运行状态。并且Zookeeper负责Region和Region Server的注册。其实Zookeeper发展到目前为止，已经成为了分布式大数据框架中容错性的标准框架。不光是HBase，几乎所有的分布式大数据相关的开源框架，都依赖于Zookeeper实现HA。</p><h1 id="2-Hbase数据模型"><a href="#2-Hbase数据模型" class="headerlink" title="2. Hbase数据模型"></a>2. Hbase数据模型</h1><p>基本概念：</p><h2 id="2-1-逻辑视图"><a href="#2-1-逻辑视图" class="headerlink" title="2.1 逻辑视图"></a>2.1 逻辑视图</h2><ol><li>RowKey：是Byte array，是表中每条记录的“主键”，方便快速查找，Rowkey的设计非常重要；</li><li>Column Family：列族，拥有一个名称(string)，包含一个或者多个相关列；</li><li>Column：属于某一个columnfamily，familyName:columnName，每条记录可动态添加；</li><li>Version Number：类型为Long，默认值是系统时间戳，可由用户自定义；</li><li>Value(Cell)：Byte array。</li></ol><h2 id="2-2-物理模型："><a href="#2-2-物理模型：" class="headerlink" title="2.2 物理模型："></a>2.2 物理模型：</h2><ul><li>每个column family存储在HDFS上的一个单独文件中，空值不会被保存。</li><li>Key 和 Version number在每个column family中均有一份；</li><li>HBase为每个值维护了多级索引，即：&lt;key, columnfamily, columnname, timestamp&gt;；</li><li>表在行的方向上分割为多个Region；</li><li>Region是Hbase中分布式存储和负载均衡的最小单元，不同Region分布到不同RegionServer上。</li><li>Region按大小分割的，随着数据增多，Region不断增大，当增大到一个阀值的时候，Region就会分成两个新的Region；</li><li>Region虽然是分布式存储的最小单元，但并不是存储的最小单元。每个Region包含着多个Store对象。每个Store包含一个MemStore或若干StoreFile，StoreFile包含一个或多个HFile。MemStore存放在内存中，StoreFile存储在HDFS上。</li><li></li></ul><h2 id="2-3-ROOT表和META表"><a href="#2-3-ROOT表和META表" class="headerlink" title="2.3 ROOT表和META表"></a>2.3 ROOT表和META表</h2><p>HBase的所有Region元数据被存储在.META.表中，随着Region的增多，.META.表中的数据也会增大，并分裂成多个新的Region。为了定位.META.表中各个Region的位置，把.META.表中所有Region的元数据保存在-ROOT-表中，最后由Zookeeper记录-ROOT-表的位置信息。所有客户端访问用户数据前，需要首先访问Zookeeper获得-ROOT-的位置，然后访问-ROOT-表获得.META.表的位置，最后根据.META.表中的信息确定用户数据存放的位置</p><p>-ROOT-表永远不会被分割，它只有一个Region，这样可以保证最多只需要三次跳转就可以定位任意一个Region。为了加快访问速度，.META.表的所有Region全部保存在内存中。客户端会将查询过的位置信息缓存起来，且缓存不会主动失效。如果客户端根据缓存信息还访问不到数据，则询问相关.META.表的Region服务器，试图获取数据的位置，如果还是失败，则询问-ROOT-表相关的.META.表在哪里。最后，如果前面的信息全部失效，则通过ZooKeeper重新定位Region的信息。所以如果客户端上的缓存全部是失效，则需要进行6次网络来回，才能定位到正确的Region。</p><h1 id="3-高可用"><a href="#3-高可用" class="headerlink" title="3. 高可用"></a>3. 高可用</h1><h2 id="3-1-Write-Ahead-Log（WAL）保障数据高可用"><a href="#3-1-Write-Ahead-Log（WAL）保障数据高可用" class="headerlink" title="3.1 Write-Ahead-Log（WAL）保障数据高可用"></a>3.1 Write-Ahead-Log（WAL）保障数据高可用</h2><p>我们理解下HLog的作用。HBase中的HLog机制是WAL的一种实现，而WAL（一般翻译为预写日志）是事务机制中常见的一致性的实现方式。每个RegionServer中都会有一个HLog的实例，RegionServer会将更新操作（如 Put，Delete）先记录到 WAL（也就是HLo）中，然后将其写入到Store的MemStore，最终MemStore会将数据写入到持久化的HFile中（MemStore 到达配置的内存阀值）。这样就保证了HBase的写的可靠性。如果没有 WAL，当RegionServer宕掉的时候，MemStore 还没有写入到HFile，或者StoreFile还没有保存，数据就会丢失。或许有的读者会担心HFile本身会不会丢失，这是由 HDFS 来保证的。在HDFS中的数据默认会有3份。因此这里并不考虑 HFile 本身的可靠性。</p><p>HFile由很多个数据块（Block）组成，并且有一个固定的结尾块。其中的数据块是由一个Header和多个Key-Value的键值对组成。在结尾的数据块中包含了数据相关的索引信息，系统也是通过结尾的索引信息找到HFile中的数据。</p><p>3.2 组件高可用</p><ul><li>Master容错：Zookeeper重新选择一个新的Master。如果无Master过程中，数据读取仍照常进行，但是，region切分、负载均衡等无法进行；</li><li>RegionServer容错：定时向Zookeeper汇报心跳，如果一旦时间内未出现心跳，Master将该RegionServer上的Region重新分配到其他RegionServer上，失效服务器上“预写”日志由主服务器进行分割并派送给新的RegionServer；</li><li>Zookeeper容错：Zookeeper是一个可靠地服务，一般配置3或5个Zookeeper实例。</li></ul><h1 id="4-HBase读写流程"><a href="#4-HBase读写流程" class="headerlink" title="4. HBase读写流程"></a>4. HBase读写流程</h1><p>HBase使用MemStore和StoreFile存储对表的更新。数据在更新时首先写入HLog和MemStore。MemStore中的数据是排序的，当MemStore累计到一定阈值时，就会创建一个新的MemStore，并且将老的MemStore添加到Flush队列，由单独的线程Flush到磁盘上，成为一个StoreFile。与此同时，系统会在Zookeeper中记录一个CheckPoint，表示这个时刻之前的数据变更已经持久化了。当系统出现意外时，可能导致MemStore中的数据丢失，此时使用HLog来恢复CheckPoint之后的数据。<br>StoreFile是只读的，一旦创建后就不可以再修改。因此Hbase的更新其实是不断追加的操作。当一个Store中的StoreFile达到一定阈值后，就会进行一次合并操作,将对同一个key的修改合并到一起，形成一个大的StoreFile。当StoreFile的大小达到一定阈值后，又会对 StoreFile进行切分操作，等分为两个StoreFile。</p><h2 id="4-1-写操作流程"><a href="#4-1-写操作流程" class="headerlink" title="4.1 写操作流程"></a>4.1 写操作流程</h2><ul><li>(1) Client通过Zookeeper的调度，向RegionServer发出写数据请求，在Region中写数据。</li><li>(2) 数据被写入Region的MemStore，直到MemStore达到预设阈值。</li><li>(3) MemStore中的数据被Flush成一个StoreFile。</li><li>(4) 随着StoreFile文件的不断增多，当其数量增长到一定阈值后，触发Compact合并操作，将多个StoreFile合并成一个StoreFile，同时进行版本合并和数据删除。</li><li>(5) StoreFiles通过不断的Compact合并操作，逐步形成越来越大的StoreFile。</li><li>(6) 单个StoreFile大小超过一定阈值后，触发Split操作，把当前Region Split成2个新的Region。父Region会下线，新Split出的2个子Region会被HMaster分配到相应的RegionServer上，使得原先1个Region的压力得以分流到2个Region上。</li><li>可以看出HBase只有增添数据，所有的更新和删除操作都是在后续的Compact历程中举行的，使得用户的写操作只要进入内存就可以立刻返回，实现了HBase I/O的高机能。</li></ul><h2 id="4-2-读操作流程"><a href="#4-2-读操作流程" class="headerlink" title="4.2 读操作流程"></a>4.2 读操作流程</h2><ul><li>(1) Client访问Zookeeper，查找-ROOT-表，获取.META.表信息。</li><li>(2) 从.META.表查找，获取存放目标数据的Region信息，从而找到对应的RegionServer。</li><li>(3) 通过RegionServer获取需要查找的数据。</li><li>(4) Regionserver的内存分为MemStore和BlockCache两部分，MemStore主要用于写数据，BlockCache主要用于读数据。读请求先到MemStore中查数据，查不到就到BlockCache中查，再查不到就会到StoreFile上读，并把读的结果放入BlockCache。</li><li>寻址过程：client–&gt;Zookeeper–&gt;-ROOT-表–&gt;.META.表–&gt;RegionServer–&gt;Region–&gt;client</li><li></li></ul><p><a href="https://www.cnblogs.com/csyuan/p/6543018.html" target="_blank" rel="noopener">https://www.cnblogs.com/csyuan/p/6543018.html</a></p>]]></content>
    
    <summary type="html">
    
      HBase框架简单介绍
    
    </summary>
    
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>大数据平台CDH搭建3</title>
    <link href="http://yoursite.com/2009/07/10/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0cdh%E6%90%AD%E5%BB%BA3/"/>
    <id>http://yoursite.com/2009/07/10/大数据平台cdh搭建3/</id>
    <published>2009-07-10T13:03:58.000Z</published>
    <updated>2018-11-10T13:24:06.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>使用github pages服务搭建博客的好处有：</p><ol><li>全是静态文件，访问速度快；</li><li>免费方便，不用花一分钱就可以搭建一个自由的个人博客，不需要服务器不需要后台；</li><li>可以随意绑定自己的域名，不仔细看的话根本看不出来你的网站是基于github的；</li></ol><a id="more"></a><ol start="4"><li>数据绝对安全，基于github的版本管理，想恢复到哪个历史版本都行；</li><li>博客内容可以轻松打包、转移、发布到其它平台；</li><li>等等；</li></ol>]]></content>
    
    <summary type="html">
    
      大数据平台CDH搭建3
    
    </summary>
    
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>大数据平台CDH搭建2</title>
    <link href="http://yoursite.com/2009/06/10/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0cdh%E6%90%AD%E5%BB%BA2/"/>
    <id>http://yoursite.com/2009/06/10/大数据平台cdh搭建2/</id>
    <published>2009-06-10T13:03:58.000Z</published>
    <updated>2018-11-10T13:24:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>使用github pages服务搭建博客的好处有：</p><ol><li>全是静态文件，访问速度快；</li><li>免费方便，不用花一分钱就可以搭建一个自由的个人博客，不需要服务器不需要后台；</li><li>可以随意绑定自己的域名，不仔细看的话根本看不出来你的网站是基于github的；</li></ol><a id="more"></a><ol start="4"><li>数据绝对安全，基于github的版本管理，想恢复到哪个历史版本都行；</li><li>博客内容可以轻松打包、转移、发布到其它平台；</li><li>等等；</li></ol>]]></content>
    
    <summary type="html">
    
      大数据平台CDH搭建2
    
    </summary>
    
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>大数据平台CDH搭建</title>
    <link href="http://yoursite.com/2009/05/10/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0cdh%E6%90%AD%E5%BB%BA/"/>
    <id>http://yoursite.com/2009/05/10/大数据平台cdh搭建/</id>
    <published>2009-05-10T13:03:58.000Z</published>
    <updated>2018-11-10T13:19:56.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>使用github pages服务搭建博客的好处有：</p><ol><li>全是静态文件，访问速度快；</li><li>免费方便，不用花一分钱就可以搭建一个自由的个人博客，不需要服务器不需要后台；</li><li>可以随意绑定自己的域名，不仔细看的话根本看不出来你的网站是基于github的；</li></ol><a id="more"></a><ol start="4"><li>数据绝对安全，基于github的版本管理，想恢复到哪个历史版本都行；</li><li>博客内容可以轻松打包、转移、发布到其它平台；</li><li>等等；</li></ol>]]></content>
    
    <summary type="html">
    
      大数据平台CDH搭建
    
    </summary>
    
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
</feed>
